{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Progressiveness\" metric\n",
    "Based on document distance between treaties per year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be cleaned\n",
    "import re\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.parsing.preprocessing import strip_short\n",
    "from pyemd import emd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObject = open('../Visualisations/Stopwords_law.pkl','rb')  \n",
    "stopwords = pickle.load(fileObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for preprocessing text\n",
    "def txt_cleaner(text):\n",
    "    meaningful_words = []\n",
    "    text = text.lower() #make all lowercase\n",
    "    #text = strip_short(text, minsize=4) #remove short words\n",
    "    tokens = word_tokenize(text) # returns list of words\n",
    "    tokens = [w for w in tokens if w.isalpha()] #remove punctuation, also numbers\n",
    "    stops = [stopwords][0]\n",
    "    tokens = [w for w in tokens if not w in stops] # remove stop words\n",
    "    for item in tokens: #filter short and long words\n",
    "        if len(item) >= 3 and len(item) < 30:\n",
    "            meaningful_words.append(item)\n",
    "    #count = Counter(meaningful_words) # Count most recurrent words\n",
    "    #most_occur = count.most_common(10) # Make list with n most recurrent\n",
    "    #most_occur = [item[0] for item in most_occur] # Get rid of the counter number\n",
    "    #meaningful_words = [w for w in meaningful_words if not w in most_occur] # remove most recurrent\n",
    "    return meaningful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search directory\n",
    "directory_in_str = \"../xml/\"\n",
    "directory = os.fsencode(directory_in_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing files to dataframe..\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>year_signed</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pta_218.xml</td>\n",
       "      <td>218</td>\n",
       "      <td>Free Trade Agreement</td>\n",
       "      <td>1975</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>[denmark, ireland, great, britain, northern, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pta_230.xml</td>\n",
       "      <td>230</td>\n",
       "      <td>Free Trade Agreement</td>\n",
       "      <td>1972</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>[portuguese, desiring, consolidate, enlargemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pta_224.xml</td>\n",
       "      <td>224</td>\n",
       "      <td>Free Trade Agreement</td>\n",
       "      <td>1977</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>[lebanon, overall, contributing, lebanon, help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pta_378.xml</td>\n",
       "      <td>378</td>\n",
       "      <td>Partial Scope Agreement</td>\n",
       "      <td>2007</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>[mauritius, islamic, pakistan, islamic, pakist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pta_344.xml</td>\n",
       "      <td>344</td>\n",
       "      <td>Free Trade Agreement</td>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>[chile, chile, hereinafter, chile, desirous, f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      filename   id                     type year_signed month day  \\\n",
       "0  pta_218.xml  218     Free Trade Agreement        1975     4  28   \n",
       "1  pta_230.xml  230     Free Trade Agreement        1972     7  22   \n",
       "2  pta_224.xml  224     Free Trade Agreement        1977     5   3   \n",
       "3  pta_378.xml  378  Partial Scope Agreement        2007     7  30   \n",
       "4  pta_344.xml  344     Free Trade Agreement        2009     7  14   \n",
       "\n",
       "                                                text  \n",
       "0  [denmark, ireland, great, britain, northern, i...  \n",
       "1  [portuguese, desiring, consolidate, enlargemen...  \n",
       "2  [lebanon, overall, contributing, lebanon, help...  \n",
       "3  [mauritius, islamic, pakistan, islamic, pakist...  \n",
       "4  [chile, chile, hereinafter, chile, desirous, f...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, the text for each treaty will be in one single list\n",
    "\n",
    "# Generate base data frame\n",
    "# The 'text' column will contain list with sublists, where every sublist will contain the tokenized text of every chapter\n",
    "print(\"Preprocessing files to dataframe..\")\n",
    "text_df = pd.DataFrame(columns=['filename', 'id','type', 'year_signed', 'month', 'day', 'text'])\n",
    "for n,file in enumerate(os.listdir(directory)):\n",
    "    new_row = []\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith('.XML') or filename.endswith('.xml'):\n",
    "        new_row.append(filename)\n",
    "        tree = ET.parse(directory_in_str + filename)\n",
    "        root = tree.getroot()\n",
    "        meta = root[0]\n",
    "        body = root[1]\n",
    "        if meta.find('language').text == 'en': \n",
    "            new_row.append(meta.find('treaty_identifier').text)\n",
    "            new_row.append(meta.find('type').text)\n",
    "            new_row.append(int(meta.find('date_signed').text.split('-')[0]))\n",
    "            new_row.append(int(meta.find('date_signed').text.split('-')[1]))\n",
    "            new_row.append(int(meta.find('date_signed').text.split('-')[2]))\n",
    "            text_raw = \"\"\n",
    "            for chapter in body.iter(): \n",
    "                text_raw += chapter.text\n",
    "            text_clean = txt_cleaner(text_raw)\n",
    "            new_row.append(text_clean)\n",
    "            text_df.loc[n] = new_row\n",
    "text_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>type</th>\n",
       "      <th>year_signed</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>pta_310.xml</td>\n",
       "      <td>Customs Union</td>\n",
       "      <td>1948</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>[informationof, south, africa, govern, ment, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>pta_254.xml</td>\n",
       "      <td>Free Trade Agreement</td>\n",
       "      <td>1951</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>[republics, nicaragua, salvador, nicaragua, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>pta_110.xml</td>\n",
       "      <td>Customs Union &amp; Economic Integration Agreement</td>\n",
       "      <td>1957</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>[consolidated, version, majesty, king, belgian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>pta_188.xml</td>\n",
       "      <td>Free Trade Agreement</td>\n",
       "      <td>1958</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>[text, xxviiiof, freetrade, intigration, afric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>pta_255.xml</td>\n",
       "      <td>Free Trade Agreement</td>\n",
       "      <td>1959</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>[congo, gabon, chad, merchandise, equatorial, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename                                            type year_signed  \\\n",
       "id                                                                             \n",
       "310  pta_310.xml                                   Customs Union        1948   \n",
       "254  pta_254.xml                            Free Trade Agreement        1951   \n",
       "110  pta_110.xml  Customs Union & Economic Integration Agreement        1957   \n",
       "188  pta_188.xml                            Free Trade Agreement        1958   \n",
       "255  pta_255.xml                            Free Trade Agreement        1959   \n",
       "\n",
       "    month day                                               text  \n",
       "id                                                                \n",
       "310    12   6  [informationof, south, africa, govern, ment, s...  \n",
       "254     3   9  [republics, nicaragua, salvador, nicaragua, sa...  \n",
       "110     3  25  [consolidated, version, majesty, king, belgian...  \n",
       "188     6  10  [text, xxviiiof, freetrade, intigration, afric...  \n",
       "255     6  23  [congo, gabon, chad, merchandise, equatorial, ...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert 'date_signed' column to datetime object\n",
    "#text_df['date_signed'] = pd.to_datetime(text_df.date_signed)\n",
    "\n",
    "# Sort items by date\n",
    "text_df.sort_values(by='year_signed', inplace=True)\n",
    "\n",
    "# Set 'id' as index (verify integrity in case of duplicates)\n",
    "text_df = text_df.set_index(['id'],verify_integrity=True)\n",
    "text_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection\n",
    "Here we define the selection of treaties we want to compare against each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of unique years\n",
    "year_unique = text_df.year_signed.unique()\n",
    "# Get list of treaty types\n",
    "type_unique = text_df.type.unique()\n",
    "\n",
    "# Separate treaties by decade\n",
    "decades_slices =[[0,5],[5,13],[13,21],[21,28],[28,37],[37,47],[47,54]]\n",
    "\n",
    "# List of dataframes to iterate over\n",
    "decades_df = []\n",
    "types_df = []\n",
    "\n",
    "# Append year slices to list of dataframes\n",
    "for item in decades_slices:\n",
    "    decades_df.append(text_df[text_df.year_signed.isin(year_unique[item[0]:item[1]])])\n",
    "\n",
    "for item in type_unique:\n",
    "    types_df.append(text_df[text_df.type.isin([item])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>type</th>\n",
       "      <th>year_signed</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>pta_136.xml</td>\n",
       "      <td>Partial Scope Agreement</td>\n",
       "      <td>1967</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>[india, arab, socialist, federal, yugoslavia, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>pta_133.xml</td>\n",
       "      <td>Partial Scope Agreement</td>\n",
       "      <td>1971</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>[auspices, lxxj, geneva, authentic, texts, fre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>pta_127.xml</td>\n",
       "      <td>Partial Scope Agreement</td>\n",
       "      <td>1975</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>[asia, pacific, bangkok, recognizing, urgent, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>pta_124.xml</td>\n",
       "      <td>Partial Scope Agreement</td>\n",
       "      <td>1980</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>[theegovernments, australia, cook, islands, fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>pta_132.xml</td>\n",
       "      <td>Partial Scope Agreement</td>\n",
       "      <td>1988</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>[recognizing, key, element, strategy, collecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>pta_120.xml</td>\n",
       "      <td>Partial Scope Agreement</td>\n",
       "      <td>1991</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>[thailand, lao, people, democratic, thailand, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>pta_89.xml</td>\n",
       "      <td>Partial Scope Agreement</td>\n",
       "      <td>1993</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>[melanesian, spearhead, papua, guinea, solomon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>pta_113.xml</td>\n",
       "      <td>Partial Scope Agreement</td>\n",
       "      <td>1993</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>[saarc, sapta, people, bangladesh, bhutan, ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>pta_117.xml</td>\n",
       "      <td>Partial Scope Agreement</td>\n",
       "      <td>2003</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>[eco, ecota, hereinafter, eco, islamic, afghan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>pta_435.xml</td>\n",
       "      <td>Partial Scope Agreement</td>\n",
       "      <td>2003</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>[india, thailand, india, thailand, hereinafter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>pta_323.xml</td>\n",
       "      <td>Partial Scope Agreement</td>\n",
       "      <td>2003</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>[india, islamic, afghanistan, india, islamic, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>pta_340.xml</td>\n",
       "      <td>Partial Scope Agreement</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>[mercosur, india, argentine, federative, brazi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>pta_374.xml</td>\n",
       "      <td>Partial Scope Agreement</td>\n",
       "      <td>2006</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>[chile, india, india, chile, hereinafter, vita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>pta_378.xml</td>\n",
       "      <td>Partial Scope Agreement</td>\n",
       "      <td>2007</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>[mauritius, islamic, pakistan, islamic, pakist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>pta_409.xml</td>\n",
       "      <td>Partial Scope Agreement</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>[argentine, federative, brazil, paraguay, orie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>pta_324.xml</td>\n",
       "      <td>Partial Scope Agreement</td>\n",
       "      <td>2009</td>\n",
       "      <td>10</td>\n",
       "      <td>27</td>\n",
       "      <td>[revised, india, delhi, oct, revised, india, n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename                     type year_signed month day  \\\n",
       "id                                                                \n",
       "136  pta_136.xml  Partial Scope Agreement        1967    12  23   \n",
       "133  pta_133.xml  Partial Scope Agreement        1971    12   8   \n",
       "127  pta_127.xml  Partial Scope Agreement        1975     7  31   \n",
       "124  pta_124.xml  Partial Scope Agreement        1980     7  14   \n",
       "132  pta_132.xml  Partial Scope Agreement        1988     4  13   \n",
       "120  pta_120.xml  Partial Scope Agreement        1991     6  20   \n",
       "89    pta_89.xml  Partial Scope Agreement        1993     7  22   \n",
       "113  pta_113.xml  Partial Scope Agreement        1993     4  11   \n",
       "117  pta_117.xml  Partial Scope Agreement        2003     7  17   \n",
       "435  pta_435.xml  Partial Scope Agreement        2003    10   9   \n",
       "323  pta_323.xml  Partial Scope Agreement        2003     3   6   \n",
       "340  pta_340.xml  Partial Scope Agreement        2004     1  25   \n",
       "374  pta_374.xml  Partial Scope Agreement        2006     3   8   \n",
       "378  pta_378.xml  Partial Scope Agreement        2007     7  30   \n",
       "409  pta_409.xml  Partial Scope Agreement        2008    12  15   \n",
       "324  pta_324.xml  Partial Scope Agreement        2009    10  27   \n",
       "\n",
       "                                                  text  \n",
       "id                                                      \n",
       "136  [india, arab, socialist, federal, yugoslavia, ...  \n",
       "133  [auspices, lxxj, geneva, authentic, texts, fre...  \n",
       "127  [asia, pacific, bangkok, recognizing, urgent, ...  \n",
       "124  [theegovernments, australia, cook, islands, fi...  \n",
       "132  [recognizing, key, element, strategy, collecti...  \n",
       "120  [thailand, lao, people, democratic, thailand, ...  \n",
       "89   [melanesian, spearhead, papua, guinea, solomon...  \n",
       "113  [saarc, sapta, people, bangladesh, bhutan, ind...  \n",
       "117  [eco, ecota, hereinafter, eco, islamic, afghan...  \n",
       "435  [india, thailand, india, thailand, hereinafter...  \n",
       "323  [india, islamic, afghanistan, india, islamic, ...  \n",
       "340  [mercosur, india, argentine, federative, brazi...  \n",
       "374  [chile, india, india, chile, hereinafter, vita...  \n",
       "378  [mauritius, islamic, pakistan, islamic, pakist...  \n",
       "409  [argentine, federative, brazil, paraguay, orie...  \n",
       "324  [revised, india, delhi, oct, revised, india, n...  "
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types_df[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance calculations\n",
    "+ Reduce amount of words(by substracting the most common words)\n",
    "+ Convert text from list of list of words to list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pretrained model\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format('../../../GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "# Notice this vectors are already normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pretrained model\n",
    "# Had to switch to smaller model\n",
    "model = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By time slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing from: 1961 to 1948\n",
      "Row: 310\n",
      "Row: 254\n",
      "Row: 110\n",
      "Row: 188\n",
      "Row: 255\n",
      "Row: 138\n",
      "Row: 275\n",
      "Row: 266\n",
      "Row: 175\n",
      "Row: 219\n",
      "Processing from: 1970 to 1962\n",
      "Row: 164\n",
      "Row: 242\n",
      "Row: 316\n",
      "Row: 172\n",
      "Row: 274\n",
      "Row: 174\n",
      "Row: 187\n",
      "Row: 136\n",
      "Row: 173\n",
      "Row: 227\n",
      "Row: 240\n",
      "Row: 135\n",
      "Row: 317\n",
      "Row: 241\n",
      "Row: 222\n",
      "Row: 134\n",
      "Row: 237\n",
      "Row: 226\n",
      "Processing from: 1979 to 1971\n",
      "Row: 133\n",
      "Row: 131\n",
      "Row: 130\n",
      "Row: 238\n",
      "Row: 207\n",
      "Row: 230\n",
      "Row: 213\n",
      "Row: 210\n",
      "Row: 128\n",
      "Row: 129\n",
      "Row: 217\n",
      "Row: 243\n",
      "Row: 127\n",
      "Row: 318\n",
      "Row: 218\n",
      "Row: 126\n",
      "Row: 206\n",
      "Row: 125\n",
      "Row: 224\n",
      "Row: 214\n",
      "Row: 239\n",
      "Row: 253\n",
      "Row: 319\n",
      "Processing from: 1988 to 1980\n",
      "Row: 124\n",
      "Row: 267\n",
      "Row: 122\n",
      "Row: 320\n",
      "Row: 121\n",
      "Row: 116\n",
      "Row: 132\n",
      "Row: 186\n",
      "Processing from: 1992 to 1992\n",
      "Row: 420\n",
      "Row: 431\n",
      "Row: 280\n",
      "Row: 422\n",
      "Row: 108\n",
      "Row: 281\n",
      "Row: 423\n",
      "Row: 354\n",
      "Row: 43\n",
      "Row: 94\n",
      "Row: 251\n",
      "Row: 432\n",
      "Row: 105\n",
      "Row: 279\n",
      "Row: 259\n",
      "Row: 276\n",
      "Row: 262\n",
      "Row: 424\n",
      "Row: 114\n",
      "Row: 115\n",
      "Row: 263\n",
      "Row: 277\n",
      "Row: 278\n",
      "Row: 203\n",
      "Row: 260\n",
      "Row: 258\n",
      "Row: 264\n",
      "Row: 112\n",
      "Row: 106\n",
      "Row: 250\n",
      "Row: 189\n",
      "Row: 245\n",
      "Processing from: 1993 to 1993\n",
      "Row: 244\n",
      "Row: 208\n",
      "Row: 247\n",
      "Row: 234\n",
      "Row: 421\n",
      "Row: 232\n",
      "Row: 209\n",
      "Row: 233\n",
      "Row: 212\n",
      "Row: 89\n",
      "Row: 113\n",
      "Row: 111\n",
      "Row: 32\n",
      "Row: 44\n",
      "Row: 257\n",
      "Row: 362\n",
      "Row: 231\n",
      "Row: 204\n",
      "Processing from: 1994 to 1994\n",
      "Row: 81\n",
      "Row: 360\n",
      "Row: 225\n",
      "Row: 272\n",
      "Row: 46\n",
      "Row: 107\n",
      "Row: 76\n",
      "Row: 202\n",
      "Row: 301\n",
      "Row: 90\n",
      "Row: 359\n",
      "Row: 294\n",
      "Row: 215\n",
      "Row: 355\n",
      "Row: 223\n",
      "Processing from: 1995 to 1995\n",
      "Row: 78\n",
      "Row: 353\n",
      "Row: 252\n",
      "Row: 246\n",
      "Row: 183\n",
      "Row: 95\n",
      "Row: 248\n",
      "Row: 93\n",
      "Row: 197\n",
      "Row: 192\n",
      "Row: 109\n",
      "Row: 98\n",
      "Row: 249\n",
      "Row: 261\n",
      "Row: 42\n",
      "Row: 91\n",
      "Row: 429\n",
      "Row: 84\n",
      "Row: 73\n",
      "Processing from: 1996 to 1996\n",
      "Row: 201\n",
      "Row: 297\n",
      "Row: 308\n",
      "Row: 235\n",
      "Row: 299\n",
      "Row: 77\n",
      "Row: 104\n",
      "Row: 74\n",
      "Row: 298\n",
      "Row: 305\n",
      "Row: 184\n",
      "Row: 300\n",
      "Row: 99\n",
      "Row: 103\n",
      "Row: 101\n",
      "Row: 236\n",
      "Row: 198\n",
      "Row: 199\n",
      "Row: 200\n",
      "Row: 306\n",
      "Row: 309\n",
      "Row: 96\n",
      "Row: 83\n",
      "Row: 290\n",
      "Row: 97\n",
      "Row: 41\n",
      "Processing from: 1997 to 1997\n",
      "Row: 65\n",
      "Row: 304\n",
      "Row: 191\n",
      "Row: 302\n",
      "Row: 289\n",
      "Row: 87\n",
      "Row: 205\n",
      "Row: 311\n",
      "Row: 273\n",
      "Row: 296\n",
      "Row: 256\n",
      "Row: 269\n",
      "Row: 102\n",
      "Row: 195\n",
      "Row: 61\n",
      "Row: 313\n",
      "Row: 75\n",
      "Row: 14\n",
      "Processing from: 1998 to 1998\n",
      "Row: 64\n",
      "Row: 287\n",
      "Row: 190\n",
      "Row: 312\n",
      "Row: 268\n",
      "Row: 288\n",
      "Row: 185\n",
      "Row: 307\n",
      "Row: 92\n",
      "Row: 271\n",
      "Row: 315\n",
      "Processing from: 1999 to 1999\n",
      "Row: 178\n",
      "Row: 314\n",
      "Row: 80\n",
      "Row: 82\n",
      "Row: 85\n",
      "Row: 45\n",
      "Row: 270\n",
      "Processing from: 2000 to 2000\n",
      "Row: 430\n",
      "Row: 70\n",
      "Row: 63\n",
      "Row: 194\n",
      "Row: 69\n",
      "Row: 79\n",
      "Row: 72\n",
      "Processing from: 2001 to 2001\n",
      "Row: 60\n",
      "Row: 66\n",
      "Row: 293\n",
      "Row: 68\n",
      "Row: 303\n",
      "Row: 40\n",
      "Row: 59\n",
      "Row: 177\n",
      "Row: 356\n",
      "Row: 179\n",
      "Row: 363\n",
      "Row: 67\n",
      "Row: 181\n",
      "Row: 358\n",
      "Row: 15\n",
      "Processing from: 2002 to 2002\n",
      "Row: 48\n",
      "Row: 193\n",
      "Row: 347\n",
      "Row: 370\n",
      "Row: 16\n",
      "Row: 55\n",
      "Row: 167\n",
      "Row: 58\n",
      "Row: 29\n",
      "Row: 265\n",
      "Row: 62\n",
      "Row: 56\n",
      "Row: 57\n",
      "Row: 6\n",
      "Row: 180\n",
      "Row: 282\n",
      "Processing from: 2003 to 2003\n",
      "Row: 166\n",
      "Row: 51\n",
      "Row: 53\n",
      "Row: 182\n",
      "Row: 50\n",
      "Row: 170\n",
      "Row: 326\n",
      "Row: 39\n",
      "Row: 117\n",
      "Row: 47\n",
      "Row: 169\n",
      "Row: 168\n",
      "Row: 54\n",
      "Row: 52\n",
      "Row: 295\n",
      "Row: 171\n",
      "Row: 165\n",
      "Row: 286\n",
      "Row: 435\n",
      "Row: 291\n",
      "Row: 357\n",
      "Row: 323\n",
      "Row: 176\n",
      "Row: 361\n",
      "Row: 292\n",
      "Processing from: 2004 to 2004\n",
      "Row: 162\n",
      "Row: 12\n",
      "Row: 26\n",
      "Row: 33\n",
      "Row: 13\n",
      "Row: 23\n",
      "Row: 364\n",
      "Row: 37\n",
      "Row: 21\n",
      "Row: 283\n",
      "Row: 340\n",
      "Row: 284\n",
      "Row: 36\n",
      "Row: 30\n",
      "Row: 31\n",
      "Row: 34\n",
      "Row: 196\n",
      "Row: 285\n",
      "Row: 38\n",
      "Row: 25\n",
      "Processing from: 2005 to 2005\n",
      "Row: 325\n",
      "Row: 27\n",
      "Row: 350\n",
      "Row: 8\n",
      "Row: 20\n",
      "Row: 19\n",
      "Row: 24\n",
      "Row: 9\n",
      "Row: 17\n",
      "Row: 2\n",
      "Row: 7\n",
      "Processing from: 2006 to 2006\n",
      "Row: 158\n",
      "Row: 342\n",
      "Row: 386\n",
      "Row: 4\n",
      "Row: 156\n",
      "Row: 372\n",
      "Row: 147\n",
      "Row: 348\n",
      "Row: 374\n",
      "Row: 10\n",
      "Row: 346\n",
      "Row: 139\n",
      "Row: 11\n",
      "Row: 371\n",
      "Processing from: 2007 to 2007\n",
      "Row: 157\n",
      "Row: 3\n",
      "Row: 161\n",
      "Row: 5\n",
      "Row: 1\n",
      "Row: 140\n",
      "Row: 159\n",
      "Row: 351\n",
      "Row: 377\n",
      "Row: 341\n",
      "Row: 426\n",
      "Row: 378\n",
      "Row: 149\n",
      "Processing from: 2008 to 2008\n",
      "Row: 349\n",
      "Row: 144\n",
      "Row: 409\n",
      "Row: 402\n",
      "Row: 387\n",
      "Row: 373\n",
      "Row: 400\n",
      "Row: 148\n",
      "Row: 335\n",
      "Row: 443\n",
      "Row: 384\n",
      "Row: 142\n",
      "Row: 153\n",
      "Row: 368\n",
      "Row: 336\n",
      "Row: 143\n",
      "Row: 152\n",
      "Processing from: 2009 to 2009\n",
      "Row: 337\n",
      "Row: 391\n",
      "Row: 411\n",
      "Row: 327\n",
      "Row: 332\n",
      "Row: 328\n",
      "Row: 404\n",
      "Row: 385\n",
      "Row: 324\n",
      "Row: 344\n",
      "Row: 155\n",
      "Row: 365\n",
      "Row: 331\n",
      "Row: 366\n",
      "Row: 352\n",
      "Row: 375\n",
      "Row: 334\n",
      "Processing from: 2010 to 2010\n",
      "Row: 333\n",
      "Row: 329\n",
      "Row: 389\n",
      "Row: 390\n",
      "Row: 369\n",
      "Row: 154\n",
      "Row: 403\n",
      "Row: 367\n",
      "Row: 145\n",
      "Processing from: 2011 to 2011\n",
      "Row: 393\n",
      "Row: 434\n",
      "Row: 413\n",
      "Row: 412\n",
      "Row: 394\n",
      "Row: 150\n",
      "Row: 345\n",
      "Row: 415\n",
      "Row: 418\n",
      "Row: 380\n",
      "Processing from: 2012 to 2012\n",
      "Row: 388\n",
      "Row: 399\n",
      "Row: 408\n",
      "Row: 160\n",
      "Row: 395\n",
      "Processing from: 2013 to 2013\n",
      "Row: 376\n",
      "Row: 330\n",
      "Row: 416\n",
      "Row: 442\n",
      "Row: 397\n",
      "Row: 436\n",
      "Row: 439\n",
      "Row: 405\n",
      "Processing from: 2014 to 2014\n",
      "Row: 343\n",
      "Row: 146\n",
      "Row: 428\n",
      "Row: 151\n",
      "Row: 427\n",
      "Row: 445\n",
      "Row: 419\n",
      "Row: 441\n",
      "Row: 392\n",
      "Row: 444\n",
      "Row: 440\n",
      "Processing from: 2015 to 2015\n",
      "Row: 379\n",
      "Row: 141\n",
      "Row: 398\n",
      "Row: 425\n",
      "Row: 401\n",
      "Row: 447\n",
      "Processing from: 2016 to 2016\n",
      "Row: 446\n",
      "Row: 396\n",
      "Row: 448\n",
      "Row: 449\n"
     ]
    }
   ],
   "source": [
    "dist_df = [] #this is where we will store all our distance matrices\n",
    "for dfx in list_df:\n",
    "    print(f'Processing from: {dfx.year_signed.max()} to {dfx.year_signed.min()}')\n",
    "    labels = list(dfx.index.values) #labels for dataframe\n",
    "    labels = ['id'] + labels #append 'id'\n",
    "    distances = []\n",
    "    for x in dfx.iterrows():\n",
    "        print(f'Row: {x[0]}')\n",
    "        dist_tmp = [] #temporal list for current row\n",
    "        t1 = [words for sublist in x[1][5] for words in sublist]\n",
    "        dist_tmp.append(x[0]) #append index\n",
    "        for y in dfx.iterrows():\n",
    "            t2 = [words for sublist in y[1][5] for words in sublist]\n",
    "            d = model.wmdistance(t1, t2)\n",
    "            dist_tmp.append(d)\n",
    "        distances.append(dist_tmp) #once done with row, append to total distances\n",
    "    df = pd.DataFrame.from_records(distances, columns=labels, index=['id']) #once all distances for current df are coll\n",
    "    dist_df.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance measured between all treaties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row: 310\n",
      "Row: 254\n",
      "Row: 110\n",
      "Row: 188\n",
      "Row: 255\n",
      "Row: 138\n",
      "Row: 275\n",
      "Row: 266\n",
      "Row: 175\n",
      "Row: 219\n",
      "Row: 164\n",
      "Row: 242\n",
      "Row: 316\n",
      "Row: 172\n",
      "Row: 274\n",
      "Row: 174\n",
      "Row: 187\n",
      "Row: 136\n",
      "Row: 173\n",
      "Row: 227\n",
      "Row: 240\n",
      "Row: 135\n",
      "Row: 317\n",
      "Row: 241\n",
      "Row: 222\n",
      "Row: 134\n",
      "Row: 237\n",
      "Row: 226\n",
      "Row: 133\n",
      "Row: 131\n",
      "Row: 130\n",
      "Row: 238\n",
      "Row: 207\n",
      "Row: 230\n",
      "Row: 213\n",
      "Row: 210\n",
      "Row: 128\n",
      "Row: 129\n",
      "Row: 217\n",
      "Row: 243\n",
      "Row: 127\n",
      "Row: 318\n",
      "Row: 218\n",
      "Row: 126\n",
      "Row: 206\n",
      "Row: 125\n",
      "Row: 224\n",
      "Row: 214\n",
      "Row: 239\n",
      "Row: 253\n",
      "Row: 319\n",
      "Row: 124\n",
      "Row: 267\n",
      "Row: 122\n",
      "Row: 320\n",
      "Row: 121\n",
      "Row: 116\n",
      "Row: 132\n",
      "Row: 186\n",
      "Row: 229\n",
      "Row: 211\n",
      "Row: 118\n",
      "Row: 220\n",
      "Row: 100\n",
      "Row: 216\n",
      "Row: 228\n",
      "Row: 119\n",
      "Row: 322\n",
      "Row: 120\n",
      "Row: 221\n",
      "Row: 420\n",
      "Row: 431\n",
      "Row: 280\n",
      "Row: 422\n",
      "Row: 108\n",
      "Row: 281\n",
      "Row: 423\n",
      "Row: 354\n",
      "Row: 43\n",
      "Row: 94\n",
      "Row: 251\n",
      "Row: 432\n",
      "Row: 105\n",
      "Row: 279\n",
      "Row: 259\n",
      "Row: 276\n",
      "Row: 262\n",
      "Row: 424\n",
      "Row: 114\n",
      "Row: 115\n",
      "Row: 263\n",
      "Row: 277\n",
      "Row: 278\n",
      "Row: 203\n",
      "Row: 260\n",
      "Row: 258\n",
      "Row: 264\n",
      "Row: 112\n",
      "Row: 106\n",
      "Row: 250\n",
      "Row: 189\n",
      "Row: 245\n",
      "Row: 244\n",
      "Row: 208\n",
      "Row: 247\n",
      "Row: 234\n",
      "Row: 421\n",
      "Row: 232\n",
      "Row: 209\n",
      "Row: 233\n",
      "Row: 212\n",
      "Row: 89\n",
      "Row: 113\n",
      "Row: 111\n",
      "Row: 32\n",
      "Row: 44\n",
      "Row: 257\n",
      "Row: 362\n",
      "Row: 231\n",
      "Row: 204\n",
      "Row: 81\n",
      "Row: 360\n",
      "Row: 225\n",
      "Row: 272\n",
      "Row: 46\n",
      "Row: 107\n",
      "Row: 76\n",
      "Row: 202\n",
      "Row: 301\n",
      "Row: 90\n",
      "Row: 359\n",
      "Row: 294\n",
      "Row: 215\n",
      "Row: 355\n",
      "Row: 223\n",
      "Row: 78\n",
      "Row: 353\n",
      "Row: 252\n",
      "Row: 246\n",
      "Row: 183\n",
      "Row: 95\n",
      "Row: 248\n",
      "Row: 93\n",
      "Row: 197\n",
      "Row: 192\n",
      "Row: 109\n",
      "Row: 98\n",
      "Row: 249\n",
      "Row: 261\n",
      "Row: 42\n",
      "Row: 91\n",
      "Row: 429\n",
      "Row: 84\n",
      "Row: 73\n",
      "Row: 201\n",
      "Row: 297\n",
      "Row: 308\n",
      "Row: 235\n",
      "Row: 299\n",
      "Row: 77\n",
      "Row: 104\n",
      "Row: 74\n",
      "Row: 298\n",
      "Row: 305\n",
      "Row: 184\n",
      "Row: 300\n",
      "Row: 99\n",
      "Row: 103\n",
      "Row: 101\n",
      "Row: 236\n",
      "Row: 198\n",
      "Row: 199\n",
      "Row: 200\n",
      "Row: 306\n",
      "Row: 309\n",
      "Row: 96\n",
      "Row: 83\n",
      "Row: 290\n",
      "Row: 97\n",
      "Row: 41\n",
      "Row: 65\n",
      "Row: 304\n",
      "Row: 191\n",
      "Row: 302\n",
      "Row: 289\n",
      "Row: 87\n",
      "Row: 205\n",
      "Row: 311\n",
      "Row: 273\n",
      "Row: 296\n",
      "Row: 256\n",
      "Row: 269\n",
      "Row: 102\n",
      "Row: 195\n",
      "Row: 61\n",
      "Row: 313\n",
      "Row: 75\n",
      "Row: 14\n",
      "Row: 64\n",
      "Row: 287\n",
      "Row: 190\n",
      "Row: 312\n",
      "Row: 268\n",
      "Row: 288\n",
      "Row: 185\n",
      "Row: 307\n",
      "Row: 92\n",
      "Row: 271\n",
      "Row: 315\n",
      "Row: 178\n",
      "Row: 314\n",
      "Row: 80\n",
      "Row: 82\n",
      "Row: 85\n",
      "Row: 45\n",
      "Row: 270\n",
      "Row: 430\n",
      "Row: 70\n",
      "Row: 63\n",
      "Row: 194\n",
      "Row: 69\n",
      "Row: 79\n",
      "Row: 72\n",
      "Row: 60\n",
      "Row: 66\n",
      "Row: 293\n",
      "Row: 68\n",
      "Row: 303\n",
      "Row: 40\n",
      "Row: 59\n",
      "Row: 177\n",
      "Row: 356\n",
      "Row: 179\n",
      "Row: 363\n",
      "Row: 67\n",
      "Row: 181\n",
      "Row: 358\n",
      "Row: 15\n",
      "Row: 48\n",
      "Row: 193\n",
      "Row: 347\n",
      "Row: 370\n",
      "Row: 16\n",
      "Row: 55\n",
      "Row: 167\n",
      "Row: 58\n",
      "Row: 29\n",
      "Row: 265\n",
      "Row: 62\n",
      "Row: 56\n",
      "Row: 57\n",
      "Row: 6\n",
      "Row: 180\n",
      "Row: 282\n",
      "Row: 166\n",
      "Row: 51\n",
      "Row: 53\n",
      "Row: 182\n",
      "Row: 50\n",
      "Row: 170\n",
      "Row: 326\n",
      "Row: 39\n",
      "Row: 117\n",
      "Row: 47\n",
      "Row: 169\n",
      "Row: 168\n",
      "Row: 54\n",
      "Row: 52\n",
      "Row: 295\n",
      "Row: 171\n",
      "Row: 165\n",
      "Row: 286\n",
      "Row: 435\n",
      "Row: 291\n",
      "Row: 357\n",
      "Row: 323\n",
      "Row: 176\n",
      "Row: 361\n",
      "Row: 292\n",
      "Row: 162\n",
      "Row: 12\n",
      "Row: 26\n",
      "Row: 33\n",
      "Row: 13\n",
      "Row: 23\n",
      "Row: 364\n",
      "Row: 37\n",
      "Row: 21\n",
      "Row: 283\n",
      "Row: 340\n",
      "Row: 284\n",
      "Row: 36\n",
      "Row: 30\n",
      "Row: 31\n",
      "Row: 34\n",
      "Row: 196\n",
      "Row: 285\n",
      "Row: 38\n",
      "Row: 25\n",
      "Row: 325\n",
      "Row: 27\n",
      "Row: 350\n",
      "Row: 8\n",
      "Row: 20\n",
      "Row: 19\n",
      "Row: 24\n",
      "Row: 9\n",
      "Row: 17\n",
      "Row: 2\n",
      "Row: 7\n",
      "Row: 158\n",
      "Row: 342\n",
      "Row: 386\n",
      "Row: 4\n",
      "Row: 156\n",
      "Row: 372\n",
      "Row: 147\n",
      "Row: 348\n",
      "Row: 374\n",
      "Row: 10\n",
      "Row: 346\n",
      "Row: 139\n",
      "Row: 11\n",
      "Row: 371\n",
      "Row: 157\n",
      "Row: 3\n",
      "Row: 161\n",
      "Row: 5\n",
      "Row: 1\n",
      "Row: 140\n",
      "Row: 159\n",
      "Row: 351\n",
      "Row: 377\n",
      "Row: 341\n",
      "Row: 426\n",
      "Row: 378\n",
      "Row: 149\n",
      "Row: 349\n",
      "Row: 144\n",
      "Row: 409\n",
      "Row: 402\n",
      "Row: 387\n",
      "Row: 373\n",
      "Row: 400\n",
      "Row: 148\n",
      "Row: 335\n",
      "Row: 443\n",
      "Row: 384\n",
      "Row: 142\n",
      "Row: 153\n",
      "Row: 368\n",
      "Row: 336\n",
      "Row: 143\n",
      "Row: 152\n",
      "Row: 337\n",
      "Row: 391\n",
      "Row: 411\n",
      "Row: 327\n",
      "Row: 332\n",
      "Row: 328\n",
      "Row: 404\n",
      "Row: 385\n",
      "Row: 324\n",
      "Row: 344\n",
      "Row: 155\n",
      "Row: 365\n",
      "Row: 331\n",
      "Row: 366\n",
      "Row: 352\n",
      "Row: 375\n",
      "Row: 334\n",
      "Row: 333\n",
      "Row: 329\n",
      "Row: 389\n",
      "Row: 390\n",
      "Row: 369\n",
      "Row: 154\n",
      "Row: 403\n",
      "Row: 367\n",
      "Row: 145\n",
      "Row: 393\n",
      "Row: 434\n",
      "Row: 413\n",
      "Row: 412\n",
      "Row: 394\n",
      "Row: 150\n",
      "Row: 345\n",
      "Row: 415\n",
      "Row: 418\n",
      "Row: 380\n",
      "Row: 388\n",
      "Row: 399\n",
      "Row: 408\n",
      "Row: 160\n",
      "Row: 395\n",
      "Row: 376\n",
      "Row: 330\n",
      "Row: 416\n",
      "Row: 442\n",
      "Row: 397\n",
      "Row: 436\n",
      "Row: 439\n",
      "Row: 405\n",
      "Row: 343\n",
      "Row: 146\n",
      "Row: 428\n",
      "Row: 151\n",
      "Row: 427\n",
      "Row: 445\n",
      "Row: 419\n",
      "Row: 441\n",
      "Row: 392\n",
      "Row: 444\n",
      "Row: 440\n",
      "Row: 379\n",
      "Row: 141\n",
      "Row: 398\n",
      "Row: 425\n",
      "Row: 401\n",
      "Row: 447\n",
      "Row: 446\n",
      "Row: 396\n",
      "Row: 448\n",
      "Row: 449\n"
     ]
    }
   ],
   "source": [
    "total_dist_df = [] #this is where we will store all our distance matrices\n",
    "\n",
    "#print(f'Processing from: {dfx.year_signed.max()} to {dfx.year_signed.min()}')\n",
    "labels = list(text_df.index.values) #labels for dataframe\n",
    "labels = ['id'] + labels #append 'id'\n",
    "distances = []\n",
    "for x in text_df.iterrows():\n",
    "    print(f'Row: {x[0]}')\n",
    "    dist_tmp = [] #temporal list for current row\n",
    "    t1 = [words for sublist in x[1][5] for words in sublist]\n",
    "    dist_tmp.append(x[0]) #append index\n",
    "    for y in text_df.iterrows():\n",
    "        t2 = [words for sublist in y[1][5] for words in sublist]\n",
    "        d = model.wmdistance(t1, t2)\n",
    "        dist_tmp.append(d)\n",
    "    distances.append(dist_tmp) #once done with row, append to total distances\n",
    "total_dist_df = pd.DataFrame.from_records(distances, columns=labels, index=['id']) #once all distances for current df are coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save resulting dataframe to csv file\n",
    "#out_dir = \"saved_csv/glove-wiki-gigaword-300/total_ollie_stopwords/\"\n",
    "\n",
    "#csv_out = \"total_dist_df.csv\"\n",
    "#total_dist_df.to_csv(out_dir + csv_out, index = True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Force directed\n",
    "+ Based on this: https://stackoverflow.com/questions/13513455/drawing-a-graph-or-a-network-from-a-distance-matrix\n",
    "+ Uses NEATO for finding the layout of nodes https://www.graphviz.org/pdf/neatoguide.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "import string\n",
    "import pygraphviz\n",
    "from networkx.readwrite import json_graph\n",
    "from networkx.drawing.nx_pydot import write_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization directory output\n",
    "out_dir_vis = \"vis/glove-wiki-gigaword-300/total/\"\n",
    "file_out = \"total_1.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = [('len', float)]\n",
    "A = total_dist_df.values*40\n",
    "A = A.view(dt)\n",
    "\n",
    "G = nx.from_numpy_matrix(A)\n",
    "G = nx.relabel_nodes(G, dict(zip(range(len(G.nodes())),list(dist_df[1].columns.values))))  #Rename node to treaty id\n",
    "G = nx.drawing.nx_agraph.to_agraph(G) # Create pygraphviz\n",
    "\n",
    "G.node_attr.update(fillcolor=\"transparent\", shape=\"circle\", style=\"filled\", width=\"0.5\")\n",
    "G.edge_attr.update(color=\"transparent\", width=\"2.0\", len=\"10\")\n",
    "G.graph_attr.update(size=\"10\",dpi='300')\n",
    "G.graph_attr['label']='Name of graph'\n",
    "\n",
    "G.draw(out_dir_vis + file_out, format='png', prog='neato')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import string\n",
    "import pygraphviz\n",
    "from networkx.readwrite import json_graph\n",
    "import numpy as np\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "from networkx.drawing.nx_pydot import write_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.layout(prog = 'neato')\n",
    "G.draw('file.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "node=G.get_node('200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Output.txt\", \"w\") as text_file:\n",
    "    print(s, file=text_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
